# Import the needed modules.
import numpy as np
import pandas as pd
import sklearn as skl
import tensorflow as tf





# Read the CSV file into a Pandas DataFrame.
telecom_df = pd.read_csv("Resources/WA_Fn-UseC_-Telco-Customer-Churn.csv") 

# Review the DataFrame.
telecom_df.head()


# Drop the non-useful 'customerID' column.
telecom_df = telecom_df.drop(["customerID"],axis=1)
telecom_df.head(3)


# Check data types of columns.
# Object type columns will need to be converted to numeric type.
telecom_df.info()


# Replace values in 'Churn' column from "No" and "Yes" to "0" and "1".
# This will turn 'Churn' into an int64 data type.
telecom_cleaned = telecom_df.replace({'Churn': {'No': 0, 'Yes': 1}})


# Replace empty space string cells with 0 in 'TotalCharges'.
# Preparation to turn this column into numeric data type.
telecom_cleaned = telecom_cleaned.replace({'TotalCharges': {' ': 0}})


# Change data type of 'TotalCharges' to Float.
telecom_cleaned['TotalCharges'] = telecom_cleaned['TotalCharges'].apply(pd.to_numeric)


# Check data type of columns to varify changes from cleaning.
telecom_cleaned.info()


# Use 'get_dummies' method to transform categorical columns into boolean columns that can be fed into deep learning model.
telecom_dummies_df = pd.get_dummies(telecom_cleaned)
telecom_dummies_df.head()


# Save cleaned dataframe to new csv file in "Resources" folder
telecom_dummies_df.to_csv('Resources/telecom_churn_cleaned.csv')





# Separate the data into dependent and independent variables.
# Separate the y variable, the dependent variable.
y = telecom_dummies_df.Churn

# Separate the X variable, the independent variables.
X = telecom_dummies_df.drop(columns="Churn")


# Review the first five data points of y variable Series.
y[:5]


# Review the first five rows of X variable DataFrame.
X.head()


# Check the balance of our target values.
y.value_counts()


# Import the train_test_learn module.
from sklearn.model_selection import train_test_split

# Split the data using train_test_split.
# Split data into train and test set using default 75/25 split.
# Assign a random_state of 42 to the function so we can consistently reproduce results.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


# Import StandardScaler 
from sklearn.preprocessing import StandardScaler

# Create StandardScaler instances.
scaler = StandardScaler()

# Fit the StandardScaler.
telecom_scaler = scaler.fit(X_train)

# Scale the data.
X_train_scaled = telecom_scaler.transform(X_train)
X_test_scaled = telecom_scaler.transform(X_test)





# Create a method that creates a new Sequential model with hyperparameter options.
def create_model(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function works best for hidden layers.
    activation = hp.Choice('activation',['relu','tanh'])
    
    # Allow kerastuner to decide number of neurons in input layer.
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=1,
        max_value=30,
        step=5), activation=activation, input_dim=45))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers.
    for i in range(hp.Int('num_layers', 1, 5)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=1,
            max_value=30,
            step=5),
            activation=activation))
    
    # Use 'sigmoid' activation as predetermined for output layer
    nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # Compile the tuner model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])
    
    return nn_model


# Import the kerastuner library
import keras_tuner as kt

# Instantiate tuner to run through 100 epochs to search for neural network model that is most accurate.
tuner = kt.Hyperband(
    create_model,
    objective="val_accuracy",
    max_epochs=100,
    hyperband_iterations=2)


# Run the kerastuner and search for best hyperparameters for neural network model.
tuner.search(X_train_scaled,y_train,epochs=100,validation_data=(X_test_scaled,y_test))


# Get top 3 model hyperparameters and print the values to use to construct.
top_hyper = tuner.get_best_hyperparameters(3)
for param in top_hyper:
    print(param.values)


# Evaluate the accuracy of top 3 models against the test dataset.
top_model = tuner.get_best_models(3)
for model in top_model:
    model_loss, model_accuracy = model.evaluate(X_test_scaled,y_test,verbose=2)
    print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Display the best model hyperparameters.
# We will be using these below for our deep learning model.
first_hyper = tuner.get_best_hyperparameters(2)[0]
first_hyper.values





# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
input_features_number = 45 # Determined by number of columns in X dataframe
input_layer = 1 # Determined by number of 'num_layers' output of tuner
hidden_layer0 = 1 # Determined by 'units_0' output of tuner
hidden_layer1 = 26 # Determined by 'units_1' output of tuner
hidden_layer2 = 11 # Determined by 'units_2' output of tuner

nn = tf.keras.models.Sequential()

# Input layer
nn.add(
    tf.keras.layers.Dense(units=input_layer, input_dim=input_features_number, activation="tanh") # use 'tanh' for input layer
)

# First hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_layer0, activation="tanh")) # use 'tanh' for hidden layer

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_layer1, activation="tanh")) # use 'tanh' for hidden layer

# Third hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_layer2, activation="tanh")) # use 'tanh' for hidden layer

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid")) # use 'sigmoid' for output layer

# Check the structure of the model.
nn.summary()


# Compile the model.
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics = [tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.BinaryAccuracy()])


# Train the model.
# Run model through 100 epochs.
fit_model = nn.fit(X_train_scaled, y_train, epochs=100)


#Evaluate the model for accuracy using the test data.
model_loss, model_precision, model_recall, model_binary_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Precision: {model_precision}, Recall: {model_recall}, Accuracy: {model_binary_accuracy}")


# Manually calculate F1 Score
print(2*(model_binary_accuracy*model_recall)/(model_binary_accuracy+model_recall))


# Export neural network model to keras file
nn.save("Resources/Telecom_Churn_Model.keras")



